{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Librabries\n",
    "\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic=  mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable =False \n",
    "    results = model.process(image)\n",
    "    image.flags.writeable =True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##rendering Landmarks \n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks2(image,result):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                              mp_drawing.DrawingSpec(color=(80,110,10),thickness=1,circle_radius =1 ),\n",
    "                              mp_drawing.DrawingSpec(color=(80,110,10),thickness=1,circle_radius =1 )\n",
    "                              \n",
    "                             \n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,22,10),thickness=2,circle_radius =1 ),\n",
    "                              mp_drawing.DrawingSpec(color=(80,44,10),thickness=2,circle_radius =1 )\n",
    "                              )\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,22,10),thickness=2,circle_radius =1 ),\n",
    "                              mp_drawing.DrawingSpec(color=(80,44,10),thickness=2,circle_radius =1 )\n",
    "                              )\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,117,10),thickness=2,circle_radius =1 ),\n",
    "                              mp_drawing.DrawingSpec(color=(80,66,10),thickness=2,circle_radius =1 )\n",
    "                              )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tracking Face and Hands\n",
    "i = 0\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        #Using Mediapipe\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)   \n",
    "        \n",
    "        #Drawing Ladmarks\n",
    "        draw_landmarks2(image,results)\n",
    "        \n",
    "        \n",
    "        cv2.imshow(\"frame\" , image)\n",
    "       # cv2.imwrite(\"image.jpg\", image)\n",
    "        if cv2.waitKey(1)==ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f97614e862fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_keypoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "##Extracting Landmarks \n",
    "\"\"\"pose =[]\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x,res.y,res.z,res.visibility])\n",
    "    pose.append(test)\"\"\"\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    \n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    \n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    \n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "test = extract_keypoints(results)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"mp_data/\")\n",
    "actions= np.array(['Thankyou', \"Hello\", \"Which\"])\n",
    "no_sequences =30 #30 Videos for each actions\n",
    "sequence_length = 30 ##30 Frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making Folders \n",
    "\n",
    "for action in actions:\n",
    "    for s in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(data_path, action, str(s)))\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "##collecting images\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        for s in range(no_sequences):\n",
    "            for i in range(sequence_length):\n",
    "                \n",
    "            \n",
    "        \n",
    "                ret, frame = cap.read()\n",
    "                #Using Mediapipe\n",
    "\n",
    "                image, results = mediapipe_detection(frame, holistic)   \n",
    "\n",
    "                #Drawing Ladmarks\n",
    "                draw_landmarks2(image,results)\n",
    "\n",
    "                if i==0:\n",
    "                    cv2.putText(image, \"Starting\", (120,200), cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),4,cv2.LINE_AA)\n",
    "                    cv2.putText(image, \"Collecting Fraes for  {} Video Number {}\".format(action,s ), (15,12), cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "                    cv2.waitKey(1000)\n",
    "                else:\n",
    "                    cv2.putText(image, \"Collecting Frames for  {} Video Number {}\".format(action,s), (15,12), cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(data_path, action,str(s),str(i))\n",
    "                np.save(npy_path,keypoints)\n",
    "                cv2.imshow(\"frame\" , image)\n",
    "\n",
    "                # cv2.imwrite(\"image.jpg\", image)\n",
    "                if cv2.waitKey(1)==ord('q'):\n",
    "                    break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thankyou': 0, 'Hello': 1, 'Which': 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "label_map = {label:num for num,label in enumerate(actions)}\n",
    "\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 30, 1662)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences, labels = [],[]\n",
    "for action in actions:\n",
    "    for seq in range(no_sequences):\n",
    "        window=[]\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(data_path,action , str(seq),\"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "        \n",
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 30, 1662)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(labels).astype(int)\n",
    "X = np.array(sequences)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##SPlitting Data\n",
    "\n",
    "X_train , X_test, y_train, y_test = train_test_split(X,y,test_size = 0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\"logs\")\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85 samples\n",
      "Epoch 1/75\n",
      "85/85 [==============================] - 3s 35ms/sample - loss: 2.7868 - categorical_accuracy: 0.3412\n",
      "Epoch 2/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 1.3642 - categorical_accuracy: 0.1882\n",
      "Epoch 3/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 1.3514 - categorical_accuracy: 0.3059\n",
      "Epoch 4/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 1.1269 - categorical_accuracy: 0.2824\n",
      "Epoch 5/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 1.3590 - categorical_accuracy: 0.3059\n",
      "Epoch 6/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 1.4694 - categorical_accuracy: 0.3176\n",
      "Epoch 7/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 2.3929 - categorical_accuracy: 0.2118\n",
      "Epoch 8/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 1.6803 - categorical_accuracy: 0.3176\n",
      "Epoch 9/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 1.2305 - categorical_accuracy: 0.1882\n",
      "Epoch 10/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 1.1566 - categorical_accuracy: 0.4235\n",
      "Epoch 11/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 1.0650 - categorical_accuracy: 0.4118\n",
      "Epoch 12/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 1.0850 - categorical_accuracy: 0.4471\n",
      "Epoch 13/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 1.1360 - categorical_accuracy: 0.5294\n",
      "Epoch 14/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 1.1196 - categorical_accuracy: 0.5765\n",
      "Epoch 15/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 1.0688 - categorical_accuracy: 0.5765\n",
      "Epoch 16/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 1.0421 - categorical_accuracy: 0.5412\n",
      "Epoch 17/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 1.0037 - categorical_accuracy: 0.5882\n",
      "Epoch 18/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.9397 - categorical_accuracy: 0.5647\n",
      "Epoch 19/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.8855 - categorical_accuracy: 0.6000\n",
      "Epoch 20/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.8391 - categorical_accuracy: 0.6118\n",
      "Epoch 21/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.8285 - categorical_accuracy: 0.5647\n",
      "Epoch 22/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.8289 - categorical_accuracy: 0.5412\n",
      "Epoch 23/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.7764 - categorical_accuracy: 0.6353\n",
      "Epoch 24/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.6527 - categorical_accuracy: 0.6824\n",
      "Epoch 25/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.6255 - categorical_accuracy: 0.6824\n",
      "Epoch 26/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.6950 - categorical_accuracy: 0.6471\n",
      "Epoch 27/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.9874 - categorical_accuracy: 0.6235\n",
      "Epoch 28/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 0.8885 - categorical_accuracy: 0.5529\n",
      "Epoch 29/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 0.9838 - categorical_accuracy: 0.4471\n",
      "Epoch 30/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 0.8348 - categorical_accuracy: 0.5647\n",
      "Epoch 31/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 0.8426 - categorical_accuracy: 0.4706\n",
      "Epoch 32/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.6348 - categorical_accuracy: 0.5882\n",
      "Epoch 33/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 0.6791 - categorical_accuracy: 0.6471\n",
      "Epoch 34/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 0.5722 - categorical_accuracy: 0.6353\n",
      "Epoch 35/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.5803 - categorical_accuracy: 0.6706\n",
      "Epoch 36/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.6711 - categorical_accuracy: 0.7176\n",
      "Epoch 37/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.7103 - categorical_accuracy: 0.6000\n",
      "Epoch 38/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.6690 - categorical_accuracy: 0.6353\n",
      "Epoch 39/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.6636 - categorical_accuracy: 0.6235\n",
      "Epoch 40/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.5859 - categorical_accuracy: 0.6706\n",
      "Epoch 41/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.5774 - categorical_accuracy: 0.7176\n",
      "Epoch 42/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.5704 - categorical_accuracy: 0.6824\n",
      "Epoch 43/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.5343 - categorical_accuracy: 0.6706\n",
      "Epoch 44/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 0.5255 - categorical_accuracy: 0.7059\n",
      "Epoch 45/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.5210 - categorical_accuracy: 0.6824\n",
      "Epoch 46/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.5029 - categorical_accuracy: 0.7765\n",
      "Epoch 47/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.4738 - categorical_accuracy: 0.7412\n",
      "Epoch 48/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.4789 - categorical_accuracy: 0.7412\n",
      "Epoch 49/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.4338 - categorical_accuracy: 0.7882\n",
      "Epoch 50/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.4746 - categorical_accuracy: 0.7529\n",
      "Epoch 51/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.4861 - categorical_accuracy: 0.7529\n",
      "Epoch 52/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.4377 - categorical_accuracy: 0.8235\n",
      "Epoch 53/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 0.4849 - categorical_accuracy: 0.7647\n",
      "Epoch 54/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.4597 - categorical_accuracy: 0.8235\n",
      "Epoch 55/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.4371 - categorical_accuracy: 0.8118\n",
      "Epoch 56/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.4107 - categorical_accuracy: 0.8118\n",
      "Epoch 57/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.4580 - categorical_accuracy: 0.7765\n",
      "Epoch 58/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.3833 - categorical_accuracy: 0.7882\n",
      "Epoch 59/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.3338 - categorical_accuracy: 0.8471\n",
      "Epoch 60/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.3196 - categorical_accuracy: 0.8706\n",
      "Epoch 61/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 0.2485 - categorical_accuracy: 0.9294\n",
      "Epoch 62/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.2468 - categorical_accuracy: 0.9059\n",
      "Epoch 63/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.2023 - categorical_accuracy: 0.9294\n",
      "Epoch 64/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.1471 - categorical_accuracy: 0.9647\n",
      "Epoch 65/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.3287 - categorical_accuracy: 0.8824\n",
      "Epoch 66/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.5796 - categorical_accuracy: 0.7294\n",
      "Epoch 67/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 0.3999 - categorical_accuracy: 0.8824\n",
      "Epoch 68/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.3435 - categorical_accuracy: 0.8471\n",
      "Epoch 69/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 0.3304 - categorical_accuracy: 0.9059\n",
      "Epoch 70/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.2610 - categorical_accuracy: 0.9412\n",
      "Epoch 71/75\n",
      "85/85 [==============================] - 1s 7ms/sample - loss: 0.1942 - categorical_accuracy: 0.9529\n",
      "Epoch 72/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.1201 - categorical_accuracy: 0.9882\n",
      "Epoch 73/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.1041 - categorical_accuracy: 0.9647\n",
      "Epoch 74/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.1079 - categorical_accuracy: 0.9765\n",
      "Epoch 75/75\n",
      "85/85 [==============================] - 1s 6ms/sample - loss: 0.0784 - categorical_accuracy: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e89cf89988>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences = True, activation = 'relu',input_shape= (30,1662)))\n",
    "model.add(LSTM(128,return_sequences= True, activation = 'relu'))\n",
    "model.add(LSTM(64,return_sequences= False, activation = 'relu'))\n",
    "model.add(Dense(64,activation= 'relu'))\n",
    "model.add(Dense(32,activation= 'relu'))\n",
    "model.add(Dense(actions.shape[0],activation= 'softmax'))\n",
    "\n",
    "model.compile(optimizer= \"adam\", loss = \"categorical_crossentropy\",metrics=['categorical_accuracy'])\n",
    "model.fit(X_train, y_train, epochs = 75,callbacks= [tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.4557160e-07 9.9999702e-01 2.3331006e-06]\n",
      " [4.3197298e-01 1.0195378e-04 5.6792504e-01]\n",
      " [4.7548665e-06 2.1823414e-15 9.9999523e-01]\n",
      " [3.2116025e-04 7.7560718e-12 9.9967885e-01]\n",
      " [2.8446260e-07 9.9999928e-01 4.9735797e-07]]\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(X_test)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(actions[np.argmax(res[4])])\n",
    "\n",
    "print(actions[np.argmax(y_test[4])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[55,  1],\n",
       "        [ 0, 29]],\n",
       "\n",
       "       [[57,  0],\n",
       "        [ 1, 27]],\n",
       "\n",
       "       [[57,  0],\n",
       "        [ 0, 28]]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##MODEL EVALUATION\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix,accuracy_score\n",
    "yhat = model.predict(X_train)\n",
    "\n",
    "ytrue= np.argmax(y_train,axis =1 ).tolist()\n",
    "yhat = np.argmax(yhat,axis = 1 ).tolist()\n",
    "\n",
    "multilabel_confusion_matrix(ytrue, yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9882352941176471"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue , yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thankyou\n",
      "Which\n",
      "Thankyou\n",
      "Which\n",
      "Thankyou\n",
      "Hello\n",
      "Thankyou\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "##Final testing\n",
    "sequences = []#concatenating 30 frames\n",
    "sentences = []\n",
    "predictions = []\n",
    "threshold = 0.7\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        #Using Mediapipe\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)   \n",
    "        \n",
    "        #Drawing Ladmarks\n",
    "        draw_landmarks2(image,results)\n",
    "        \n",
    "        \n",
    "        ##making predictions\n",
    "        \n",
    "        keypoints = extract_keypoints(results)\n",
    "        #sequences.insert(0,keypoints)\n",
    "        sequences.append(keypoints)\n",
    "        sequences = sequences[-30:]\n",
    "        \n",
    "        if len(sequences)==30:\n",
    "            res = model.predict(np.expand_dims(sequences,axis = 0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "        \n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res):\n",
    "                if res[np.argmax(res)]>threshold:\n",
    "\n",
    "                    if len(sentences)>0:\n",
    "                        if actions[np.argmax(res)]!=sentences[-1]:\n",
    "                            sentences.append(actions[np.argmax(res)])\n",
    "                            print(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentences.append(actions[np.argmax(res)])\n",
    "            if len(sentences)>5:\n",
    "                sentences = sentences[-5:]\n",
    "\n",
    "        cv2.rectangle(image, (0,0),(640,40), (245,117,16),-1 )\n",
    "        cv2.putText(image,\"\".join(sentences),(3,30),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        cv2.imshow(\"frame\" , image)\n",
    "       # cv2.imwrite(\"image.jpg\", image)\n",
    "        if cv2.waitKey(1)==ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
